{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFOSYS722_BDAS_ASSIGNMENT_LORRAINE ZHOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('INFOSYS722').getOrCreate()\n",
    "import pandas as pd\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('BostonCrime2.0.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             11|        1107|               Fraud|    null|              |2015|    7|  Wednesday|  24|  Part Two|        COBDEN|       N|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INCIDENT_NUMBER: integer (nullable = true)\n",
      " |-- OFFENSE_CODE: integer (nullable = true)\n",
      " |-- OFFENSE_CODE_GROUP: string (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- REPORTING_AREA: string (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- HOUR: integer (nullable = true)\n",
      " |-- UCR_PART: string (nullable = true)\n",
      " |-- STREET: string (nullable = true)\n",
      " |-- SHOOTING: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(INCIDENT_NUMBER=1, OFFENSE_CODE=1102, OFFENSE_CODE_GROUP='Fraud', DISTRICT='D4', REPORTING_AREA='619', YEAR=2015, MONTH=12, DAY_OF_WEEK='Sunday', HOUR=14, UCR_PART='Part Two', STREET='WESTLAND AVE', SHOOTING='N')]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+--------+-----------------+------------------+-----------------+-----------+-----------------+--------+---------+--------+\n",
      "|summary|   INCIDENT_NUMBER|      OFFENSE_CODE|OFFENSE_CODE_GROUP|DISTRICT|   REPORTING_AREA|              YEAR|            MONTH|DAY_OF_WEEK|             HOUR|UCR_PART|   STREET|SHOOTING|\n",
      "+-------+------------------+------------------+------------------+--------+-----------------+------------------+-----------------+-----------+-----------------+--------+---------+--------+\n",
      "|  count|               800|               800|               800|     772|              800|               800|              800|        800|              800|     800|      783|     800|\n",
      "|   mean|             400.5|        1949.24875|              null|    null| 404.742782152231|        2016.50375|           8.2025|       null|            15.49|    null|     null|    null|\n",
      "| stddev|231.08440016582685|1095.3304308812276|              null|    null|253.4539993960598|1.1209623692572204|2.677770867909777|       null|6.011661591762074|    null|     null|    null|\n",
      "|    min|                 1|               301|Aggravated Assault|      A1|                 |              2015|                1|     Friday|                1|   Other| ADAMS ST|       N|\n",
      "|    max|               800|              3831|   Warrant Arrests|      E5|               97|              2018|               12|  Wednesday|               24|Part Two|ZAMORA ST|       Y|\n",
      "+-------+------------------+------------------+------------------+--------+-----------------+------------------+-----------------+-----------+-----------------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+\n",
      "|summary|              YEAR|            MONTH|             HOUR|\n",
      "+-------+------------------+-----------------+-----------------+\n",
      "|  count|               800|              800|              800|\n",
      "|   mean|        2016.50375|           8.2025|            15.49|\n",
      "| stddev|1.1209623692572204|2.677770867909777|6.011661591762074|\n",
      "|    min|              2015|                1|                1|\n",
      "|    max|              2018|               12|               24|\n",
      "+-------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('YEAR','MONTH','HOUR').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+--------+-----------------+------------------+-----------------+-----------+-----------------+--------+---------+--------+\n",
      "|summary|   INCIDENT_NUMBER|      OFFENSE_CODE|OFFENSE_CODE_GROUP|DISTRICT|   REPORTING_AREA|              YEAR|            MONTH|DAY_OF_WEEK|             HOUR|UCR_PART|   STREET|SHOOTING|\n",
      "+-------+------------------+------------------+------------------+--------+-----------------+------------------+-----------------+-----------+-----------------+--------+---------+--------+\n",
      "|  count|               800|               800|               800|     772|              800|               800|              800|        800|              800|     800|      783|     800|\n",
      "|   mean|             400.5|        1949.24875|              null|    null| 404.742782152231|        2016.50375|           8.2025|       null|            15.49|    null|     null|    null|\n",
      "| stddev|231.08440016582685|1095.3304308812276|              null|    null|253.4539993960598|1.1209623692572204|2.677770867909777|       null|6.011661591762074|    null|     null|    null|\n",
      "|    min|                 1|               301|Aggravated Assault|      A1|                 |              2015|                1|     Friday|                1|   Other| ADAMS ST|       N|\n",
      "|    max|               800|              3831|   Warrant Arrests|      E5|               97|              2018|               12|  Wednesday|               24|Part Two|ZAMORA ST|       Y|\n",
      "+-------+------------------+------------------+------------------+--------+-----------------+------------------+-----------------+-----------+-----------------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('INCIDENT_NUMBER','OFFENSE_CODE','OFFENSE_CODE_GROUP','DISTRICT','REPORTING_AREA','YEAR','MONTH','DAY_OF_WEEK','HOUR','UCR_PART','STREET','SHOOTING').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "|summary|DISTRICT|   STREET|\n",
      "+-------+--------+---------+\n",
      "|  count|     772|      783|\n",
      "|   mean|    null|     null|\n",
      "| stddev|    null|     null|\n",
      "|    min|      A1| ADAMS ST|\n",
      "|    max|      E5|ZAMORA ST|\n",
      "+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DISTRICT','STREET').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|   12|\n",
      "|   12|\n",
      "|    7|\n",
      "|   11|\n",
      "|   10|\n",
      "|    9|\n",
      "|    8|\n",
      "|   10|\n",
      "|    8|\n",
      "|    8|\n",
      "|    7|\n",
      "|   11|\n",
      "|   12|\n",
      "|    7|\n",
      "|    9|\n",
      "|    9|\n",
      "|   10|\n",
      "|    7|\n",
      "|    8|\n",
      "|   10|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mon_col = df.select('month')\n",
    "mon_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             11|        1107|               Fraud|    null|              |2015|    7|  Wednesday|  24|  Part Two|        COBDEN|       N|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-----+\n",
      "|YEAR|MONTH|\n",
      "+----+-----+\n",
      "|2015|   12|\n",
      "|2015|   12|\n",
      "|2015|    7|\n",
      "|2015|   11|\n",
      "|2015|   10|\n",
      "|2015|    9|\n",
      "|2015|    8|\n",
      "|2015|   10|\n",
      "|2015|    8|\n",
      "|2015|    8|\n",
      "|2015|    7|\n",
      "|2015|   11|\n",
      "|2015|   12|\n",
      "|2015|    7|\n",
      "|2015|    9|\n",
      "|2015|    9|\n",
      "|2015|   10|\n",
      "|2015|    7|\n",
      "|2015|    8|\n",
      "|2015|   10|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"MONTH > 6\").show()\n",
    "df.filter(\"MONTH > 6\").select('YEAR','MONTH').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|              STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------------+--------+\n",
      "|             38|        2629|          Harassment|     C11|           397|2015|    6|     Monday|  24|  Part Two|        MELBOURNE ST|       N|\n",
      "|             93|        3018|  Medical Assistance|      C6|           177|2015|    6|     Sunday|  12|Part Three|      NEW SUDBURY ST|       N|\n",
      "|             97|        1102|               Fraud|      D4|           618|2015|    6|     Friday|  17|  Part Two|            OPERA PL|       N|\n",
      "|            123|        3501|Missing Person Re...|      B3|           457|2015|    6|   Saturday|  16|Part Three|         DRUMMOND ST|       N|\n",
      "|            124|        3502|Missing Person Lo...|      B3|           457|2015|    6|   Saturday|  16|Part Three|         DRUMMOND ST|       N|\n",
      "|            132|        1102|               Fraud|      D4|           159|2015|    6|     Monday|   9|  Part Two|        DARTMOUTH ST|       N|\n",
      "|            140|        3115|  Investigate Person|     D14|           751|2015|    6|    Tuesday|  16|Part Three|       LAKE SHORE RD|       N|\n",
      "|            141|        3201|       Property Lost|      B3|           448|2015|    6|    Tuesday|  24|Part Three|          SPENCER ST|       N|\n",
      "|            142|        2629|          Harassment|      D4|           905|2015|    6|     Monday|  12|  Part Two|   HARRISON ARCHWAYS|       N|\n",
      "|            205|        1102|               Fraud|     C11|           817|2016|    6|   Thursday|  24|  Part Two|          ARGYLE TER|       N|\n",
      "|            214|        2629|          Harassment|      D4|           165|2016|    6|  Wednesday|  11|  Part Two|        HARRISON AVE|       N|\n",
      "|            218|        2629|          Harassment|     A15|            47|2016|    6|    Tuesday|   8|  Part Two|           N MEAD ST|       N|\n",
      "|            228|        3201|       Property Lost|     D14|           797|2016|    6|  Wednesday|  12|Part Three|            BUICK ST|       N|\n",
      "|            230|        2647|               Other|     E18|           923|2016|    6|  Wednesday|   8|  Part Two|           PIERCE ST|       N|\n",
      "|            241|        1102|               Fraud|     C11|           341|2016|    6|  Wednesday|   9|  Part Two|        SPEEDWELL ST|       N|\n",
      "|            245|        1102|               Fraud|      A1|           124|2016|    6|     Monday|   9|  Part Two|           HUDSON ST|       N|\n",
      "|            259|        1102|               Fraud|      B3|           943|2016|    6|   Thursday|   9|  Part Two|        WOODRUFF WAY|       N|\n",
      "|            272|        3115|  Investigate Person|     C11|           396|2016|    6|  Wednesday|   8|Part Three|          SANTUIT ST|       N|\n",
      "|            301|         616|             Larceny|      D4|           168|2016|    6|  Wednesday|  12|  Part One|FATHER FRANCIS J ...|       N|\n",
      "|            311|        1102|               Fraud|      B2|           182|2016|    6|   Saturday|  24|  Part Two|          BURRELL ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.MONTH == '6').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|              STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------------+--------+\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|         OAKCREST RD|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two|       WASHINGTON ST|       N|\n",
      "|             23|        3110|Police Service In...|      E5|           552|2015|    9|    Tuesday|  12|Part Three|          DENTON TER|       N|\n",
      "|             32|        1102|               Fraud|      A1|            92|2015|   10|    Tuesday|  12|  Part Two|            COURT ST|       N|\n",
      "|             33|        1109|               Fraud|      B2|           311|2015|   11|   Thursday|  12|  Part Two|       BLUE HILL AVE|       N|\n",
      "|             40|        1102|               Fraud|      B3|           433|2015|    9|     Monday|  12|  Part Two|            EVANS ST|       N|\n",
      "|             42|        1107|               Fraud|      B2|           292|2015|    8|   Saturday|  12|  Part Two|       JOHN ELIOT SQ|       N|\n",
      "|             44|        3201|       Property Lost|     D14|           795|2015|    9|     Friday|  12|Part Three|    COMMONWEALTH AVE|       N|\n",
      "|             54|        2647|               Other|      B2|           301|2015|   12|    Tuesday|  12|  Part Two|         HIGHLAND ST|       N|\n",
      "|             56|        3201|       Property Lost|      C6|           177|2015|    8|   Saturday|  12|Part Three|      NEW SUDBURY ST|       N|\n",
      "|             65|        1107|               Fraud|      D4|           169|2015|   12|    Tuesday|  12|  Part Two|         E NEWTON ST|       N|\n",
      "|             72|        1102|               Fraud|      C6|           914|2015|   10|   Thursday|  12|  Part Two|GENERAL LAWRENCE ...|       N|\n",
      "|             80|        1107|               Fraud|      D4|           903|2015|   11|   Saturday|  12|  Part Two|        LATTIMORE CT|       N|\n",
      "|             83|        1102|               Fraud|      C6|           219|2015|   11|     Friday|  12|  Part Two|         COLUMBIA RD|       N|\n",
      "|             84|         614|Larceny From Moto...|      B2|           327|2015|   11|    Tuesday|  12|  Part One|           QUINCY ST|       N|\n",
      "|             88|        1106|    Confidence Games|      D4|           625|2015|   10|     Monday|  12|  Part Two|     PETERBOROUGH ST|       N|\n",
      "|             90|        1107|               Fraud|      D4|           171|2015|   11|     Monday|  12|  Part Two|           ALBANY ST|       N|\n",
      "|             92|        2629|          Harassment|      B2|           180|2015|   10|  Wednesday|  12|  Part Two|          DUNMORE ST|       N|\n",
      "|             93|        3018|  Medical Assistance|      C6|           177|2015|    6|     Sunday|  12|Part Three|      NEW SUDBURY ST|       N|\n",
      "|             94|        1102|               Fraud|      E5|           550|2015|   10|   Saturday|  12|  Part Two|             JUNE ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"HOUR='12'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----+\n",
      "|INCIDENT_NUMBER|MONTH|HOUR|\n",
      "+---------------+-----+----+\n",
      "|              4|   11|   8|\n",
      "|              6|    9|  11|\n",
      "|              7|    8|   8|\n",
      "|              9|    8|   6|\n",
      "|             12|   11|  11|\n",
      "|             13|   12|   4|\n",
      "|             14|    7|   8|\n",
      "|             16|    9|   9|\n",
      "|             17|   10|   8|\n",
      "|             19|    8|   9|\n",
      "|             21|   12|   9|\n",
      "|             25|    9|   9|\n",
      "|             26|   10|   9|\n",
      "|             27|    8|   9|\n",
      "|             28|    7|   2|\n",
      "|             29|   10|   9|\n",
      "|             31|   11|   9|\n",
      "|             43|    9|   7|\n",
      "|             45|    8|   9|\n",
      "|             55|    8|   9|\n",
      "+---------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"MONTH > 6 AND HOUR < 12\").select('INCIDENT_NUMBER','MONTH','HOUR').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutate, or creating new columns (only boolean type, string is not applicable, so the result comes null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+--------------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|STREETDISTRICT|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+--------------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|          null|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|          null|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|          null|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|          null|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|          null|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|          null|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|          null|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|          null|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|          null|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|          null|\n",
      "|             11|        1107|               Fraud|    null|              |2015|    7|  Wednesday|  24|  Part Two|        COBDEN|       N|          null|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|          null|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|          null|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|          null|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|          null|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|          null|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|          null|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|          null|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|          null|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|          null|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('STREETDISTRICT', df.STREET*df.DISTRICT).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize and group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "|MONTH|avg(INCIDENT_NUMBER)| avg(OFFENSE_CODE)|         avg(YEAR)|avg(MONTH)|         avg(HOUR)|\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "|   12|   298.0091743119266| 1735.697247706422|2015.9816513761468|      12.0|15.458715596330276|\n",
      "|    1|  339.05882352941177|1959.2941176470588|2016.4117647058824|       1.0|17.529411764705884|\n",
      "|    6|  387.76785714285717| 2149.214285714286|            2016.5|       6.0|              14.5|\n",
      "|    3|               403.6|           1981.25|            2016.5|       3.0|             15.35|\n",
      "|    5|   404.9130434782609|2087.1739130434785|2016.5652173913043|       5.0|13.695652173913043|\n",
      "|    9|             275.175|         1879.8125|         2015.8375|       9.0|             16.45|\n",
      "|    4|   361.1764705882353|1483.6470588235295|2016.3529411764705|       4.0|15.647058823529411|\n",
      "|    8|  267.67857142857144| 2092.214285714286| 2015.892857142857|       8.0|15.964285714285714|\n",
      "|    7|   616.5818965517242| 2165.668103448276|2017.5905172413793|       7.0|15.387931034482758|\n",
      "|   10|   307.7752808988764|1578.9550561797753|2016.0224719101125|      10.0|14.561797752808989|\n",
      "|   11|  280.06976744186045|1918.2790697674418|2015.8255813953488|      11.0| 16.63953488372093|\n",
      "|    2|  358.93333333333334|            1880.6|2016.2666666666667|       2.0|13.466666666666667|\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('MONTH').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "|DISTRICT|avg(INCIDENT_NUMBER)| avg(OFFENSE_CODE)|         avg(YEAR)|       avg(MONTH)|         avg(HOUR)|\n",
      "+--------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "|      C6|  454.06060606060606|1832.7272727272727|2016.7272727272727|8.742424242424242|14.984848484848484|\n",
      "|    null|               586.0|2155.8571428571427|2017.5357142857142|6.642857142857143|15.821428571428571|\n",
      "|      B2|   412.8411214953271|  1844.96261682243|2016.5140186915887|8.327102803738319| 15.14018691588785|\n",
      "|     C11|  349.24074074074076|1947.4722222222222|2016.3333333333333| 8.50925925925926|15.814814814814815|\n",
      "|     E13|  488.88235294117646|2167.3529411764707|2016.9117647058824|7.764705882352941|14.941176470588236|\n",
      "|      B3|  365.25396825396825|2144.2698412698414|2016.3174603174602|7.761904761904762|15.031746031746032|\n",
      "|      E5|  360.94545454545454|1907.3090909090909|2016.3272727272727|9.145454545454545|17.927272727272726|\n",
      "|     A15|  403.42857142857144|1898.2857142857142|2016.4285714285713|6.571428571428571|15.428571428571429|\n",
      "|      A7|   365.4166666666667|1835.0833333333333|2016.2916666666667|8.041666666666666|             12.75|\n",
      "|     D14|   393.6842105263158|2020.3333333333333|2016.3508771929824|8.614035087719298| 16.56140350877193|\n",
      "|      D4|   395.7155963302752|1866.3853211009175|2016.5045871559632|8.247706422018348|15.889908256880734|\n",
      "|     E18|   363.4727272727273|2017.1454545454546|2016.3454545454545|8.381818181818181|13.581818181818182|\n",
      "|      A1|            408.3125|           1956.25|         2016.5625|           7.4625|            15.575|\n",
      "+--------+--------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('DISTRICT').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by HOUR\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "|MONTH|avg(INCIDENT_NUMBER)| avg(OFFENSE_CODE)|         avg(YEAR)|avg(MONTH)|         avg(HOUR)|\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "|    2|  358.93333333333334|            1880.6|2016.2666666666667|       2.0|13.466666666666667|\n",
      "|    5|   404.9130434782609|2087.1739130434785|2016.5652173913043|       5.0|13.695652173913043|\n",
      "|    6|  387.76785714285717| 2149.214285714286|            2016.5|       6.0|              14.5|\n",
      "|   10|   307.7752808988764|1578.9550561797753|2016.0224719101125|      10.0|14.561797752808989|\n",
      "|    3|               403.6|           1981.25|            2016.5|       3.0|             15.35|\n",
      "|    7|   616.5818965517242| 2165.668103448276|2017.5905172413793|       7.0|15.387931034482758|\n",
      "|   12|   298.0091743119266| 1735.697247706422|2015.9816513761468|      12.0|15.458715596330276|\n",
      "|    4|   361.1764705882353|1483.6470588235295|2016.3529411764705|       4.0|15.647058823529411|\n",
      "|    8|  267.67857142857144| 2092.214285714286| 2015.892857142857|       8.0|15.964285714285714|\n",
      "|    9|             275.175|         1879.8125|         2015.8375|       9.0|             16.45|\n",
      "|   11|  280.06976744186045|1918.2790697674418|2015.8255813953488|      11.0| 16.63953488372093|\n",
      "|    1|  339.05882352941177|1959.2941176470588|2016.4117647058824|       1.0|17.529411764705884|\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "\n",
      "Sorted by YEAR\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "|MONTH|avg(INCIDENT_NUMBER)| avg(OFFENSE_CODE)|         avg(YEAR)|avg(MONTH)|         avg(HOUR)|\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "|   11|  280.06976744186045|1918.2790697674418|2015.8255813953488|      11.0| 16.63953488372093|\n",
      "|    9|             275.175|         1879.8125|         2015.8375|       9.0|             16.45|\n",
      "|    8|  267.67857142857144| 2092.214285714286| 2015.892857142857|       8.0|15.964285714285714|\n",
      "|   12|   298.0091743119266| 1735.697247706422|2015.9816513761468|      12.0|15.458715596330276|\n",
      "|   10|   307.7752808988764|1578.9550561797753|2016.0224719101125|      10.0|14.561797752808989|\n",
      "|    2|  358.93333333333334|            1880.6|2016.2666666666667|       2.0|13.466666666666667|\n",
      "|    4|   361.1764705882353|1483.6470588235295|2016.3529411764705|       4.0|15.647058823529411|\n",
      "|    1|  339.05882352941177|1959.2941176470588|2016.4117647058824|       1.0|17.529411764705884|\n",
      "|    6|  387.76785714285717| 2149.214285714286|            2016.5|       6.0|              14.5|\n",
      "|    3|               403.6|           1981.25|            2016.5|       3.0|             15.35|\n",
      "|    5|   404.9130434782609|2087.1739130434785|2016.5652173913043|       5.0|13.695652173913043|\n",
      "|    7|   616.5818965517242| 2165.668103448276|2017.5905172413793|       7.0|15.387931034482758|\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_month_df = df.groupBy('MONTH').mean()\n",
    "print(\"Sorted by HOUR\")\n",
    "group_month_df.orderBy('avg(HOUR)').show()\n",
    "print(\"Sorted by YEAR\")\n",
    "df.groupBy('MONTH').mean().orderBy('avg(YEAR)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "|MONTH|avg(INCIDENT_NUMBER)| avg(OFFENSE_CODE)|         avg(YEAR)|avg(MONTH)|         avg(HOUR)|\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "|   12|   298.0091743119266| 1735.697247706422|2015.9816513761468|      12.0|15.458715596330276|\n",
      "|    1|  339.05882352941177|1959.2941176470588|2016.4117647058824|       1.0|17.529411764705884|\n",
      "|    6|  387.76785714285717| 2149.214285714286|            2016.5|       6.0|              14.5|\n",
      "|    3|               403.6|           1981.25|            2016.5|       3.0|             15.35|\n",
      "|    5|   404.9130434782609|2087.1739130434785|2016.5652173913043|       5.0|13.695652173913043|\n",
      "|    9|             275.175|         1879.8125|         2015.8375|       9.0|             16.45|\n",
      "|    4|   361.1764705882353|1483.6470588235295|2016.3529411764705|       4.0|15.647058823529411|\n",
      "|    8|  267.67857142857144| 2092.214285714286| 2015.892857142857|       8.0|15.964285714285714|\n",
      "|    7|   616.5818965517242| 2165.668103448276|2017.5905172413793|       7.0|15.387931034482758|\n",
      "|   10|   307.7752808988764|1578.9550561797753|2016.0224719101125|      10.0|14.561797752808989|\n",
      "|   11|  280.06976744186045|1918.2790697674418|2015.8255813953488|      11.0| 16.63953488372093|\n",
      "|    2|  358.93333333333334|            1880.6|2016.2666666666667|       2.0|13.466666666666667|\n",
      "+-----+--------------------+------------------+------------------+----------+------------------+\n",
      "\n",
      "+-----+---------------------------+---------------------------+\n",
      "|MONTH|format_number(avg(HOUR), 2)|format_number(avg(YEAR), 2)|\n",
      "+-----+---------------------------+---------------------------+\n",
      "|   12|                      15.46|                   2,015.98|\n",
      "|    1|                      17.53|                   2,016.41|\n",
      "|    6|                      14.50|                   2,016.50|\n",
      "|    3|                      15.35|                   2,016.50|\n",
      "|    5|                      13.70|                   2,016.57|\n",
      "|    9|                      16.45|                   2,015.84|\n",
      "|    4|                      15.65|                   2,016.35|\n",
      "|    8|                      15.96|                   2,015.89|\n",
      "|    7|                      15.39|                   2,017.59|\n",
      "|   10|                      14.56|                   2,016.02|\n",
      "|   11|                      16.64|                   2,015.83|\n",
      "|    2|                      13.47|                   2,016.27|\n",
      "+-----+---------------------------+---------------------------+\n",
      "\n",
      "+---------------+------------+------------+\n",
      "|INCIDENT_NUMBER|Average HOUR|Average YEAR|\n",
      "+---------------+------------+------------+\n",
      "|             12|       15.46|    2,015.98|\n",
      "|              1|       17.53|    2,016.41|\n",
      "|              6|       14.50|    2,016.50|\n",
      "|              3|       15.35|    2,016.50|\n",
      "|              5|       13.70|    2,016.57|\n",
      "|              9|       16.45|    2,015.84|\n",
      "|              4|       15.65|    2,016.35|\n",
      "|              8|       15.96|    2,015.89|\n",
      "|              7|       15.39|    2,017.59|\n",
      "|             10|       14.56|    2,016.02|\n",
      "|             11|       16.64|    2,015.83|\n",
      "|              2|       13.47|    2,016.27|\n",
      "+---------------+------------+------------+\n",
      "\n",
      "Average HOUR and YEAR by INCIDENT_NUMBER\n",
      "+---------------+------------+------------+\n",
      "|INCIDENT_NUMBER|Average HOUR|Average YEAR|\n",
      "+---------------+------------+------------+\n",
      "|              2|       13.47|    2,016.27|\n",
      "|              5|       13.70|    2,016.57|\n",
      "|              6|       14.50|    2,016.50|\n",
      "|             10|       14.56|    2,016.02|\n",
      "|              3|       15.35|    2,016.50|\n",
      "|              7|       15.39|    2,017.59|\n",
      "|             12|       15.46|    2,015.98|\n",
      "|              4|       15.65|    2,016.35|\n",
      "|              8|       15.96|    2,015.89|\n",
      "|              9|       16.45|    2,015.84|\n",
      "|             11|       16.64|    2,015.83|\n",
      "|              1|       17.53|    2,016.41|\n",
      "+---------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_month_df = df.groupBy('MONTH').mean()\n",
    "group_month_df.show()\n",
    "group_month_df = group_month_df.select('MONTH',\n",
    "                                   format_number('avg(HOUR)',2),\n",
    "                                   format_number('avg(YEAR)',2))\n",
    "group_month_df.show()\n",
    "\n",
    "group_month_df = group_month_df.select(col('MONTH').alias('INCIDENT_NUMBER'),\n",
    "                                   col('format_number(avg(HOUR), 2)').alias('Average HOUR'),\n",
    "                                   col('format_number(avg(YEAR), 2)').alias('Average YEAR'))\n",
    "group_month_df.show()\n",
    "                            \n",
    "group_month_df = group_month_df.orderBy('Average HOUR')\n",
    "                                       \n",
    "print('Average HOUR and YEAR by INCIDENT_NUMBER')\n",
    "group_month_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             11|        1107|               Fraud|    null|              |2015|    7|  Wednesday|  24|  Part Two|        COBDEN|       N|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 800\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('BostonCrime2.0.csv', header=True, inferSchema=True)\n",
    "df.show()\n",
    "print(\"Total data points:\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Rows - Missing Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "|             21|        1102|               Fraud|     C11|           346|2015|   12|    Tuesday|   9|  Part Two|   JULIETTE ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 800\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()\n",
    "print(\"Total data points:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "|             21|        1102|               Fraud|     C11|           346|2015|   12|    Tuesday|   9|  Part Two|   JULIETTE ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 769\n"
     ]
    }
   ],
   "source": [
    "dropped_df = df.na.drop()\n",
    "dropped_df.show()\n",
    "print(\"Total data points:\", dropped_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Rows - Missing Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             11|        1107|               Fraud|    null|              |2015|    7|  Wednesday|  24|  Part Two|        COBDEN|       N|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 783\n"
     ]
    }
   ],
   "source": [
    "missing_field_df = df.na.drop(subset=\"STREET\")\n",
    "missing_field_df.show()\n",
    "print(\"Total data points:\", missing_field_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "|             21|        1102|               Fraud|     C11|           346|2015|   12|    Tuesday|   9|  Part Two|   JULIETTE ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 772\n"
     ]
    }
   ],
   "source": [
    "missing_field_df = df.na.drop(subset=\"DISTRICT\")\n",
    "missing_field_df.show()\n",
    "print(\"Total data points:\", missing_field_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|      D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|      D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|     E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|      C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|      D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|     D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|     C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|     C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|     C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|      A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             11|        1107|               Fraud|    null|              |2015|    7|  Wednesday|  24|  Part Two|        COBDEN|       N|\n",
      "|             12|         735| Auto Theft Recovery|     C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|     C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|      A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|     C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|     C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|     C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|     D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|      A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|      D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "+---------------+------------+--------------------+--------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 800\n"
     ]
    }
   ],
   "source": [
    "filled_df = df.na.fill('UNDISCLOSED', subset=['STREET'])\n",
    "filled_df.show()\n",
    "print(\"Total data points:\", filled_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+-----------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP|   DISTRICT|REPORTING_AREA|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|        STREET|SHOOTING|\n",
      "+---------------+------------+--------------------+-----------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "|              1|        1102|               Fraud|         D4|           619|2015|   12|     Sunday|  14|  Part Two|  WESTLAND AVE|       N|\n",
      "|              2|         619|             Larceny|         D4|           619|2015|   12|     Sunday|  14|  Part One|  WESTLAND AVE|       N|\n",
      "|              3|        1107|               Fraud|        E18|           486|2015|    7|  Wednesday|  12|  Part Two|   OAKCREST RD|       N|\n",
      "|              4|        1107|               Fraud|         C6|           226|2015|   11|   Thursday|   8|  Part Two|   E FOURTH ST|       N|\n",
      "|              5|        2647|               Other|         D4|           129|2015|   10|   Saturday|  13|  Part Two|   BERKELEY ST|       N|\n",
      "|              6|        1102|               Fraud|        D14|           791|2015|    9|    Tuesday|  11|  Part Two|    ALLSTON ST|       N|\n",
      "|              7|        3201|       Property Lost|        C11|           366|2015|    8|   Thursday|   8|Part Three|      EDWIN ST|       N|\n",
      "|              8|        1102|               Fraud|        C11|           450|2015|   10|  Wednesday|  12|  Part Two| WASHINGTON ST|       N|\n",
      "|              9|        1102|               Fraud|        C11|           352|2015|    8|   Saturday|   6|  Part Two|     DITSON ST|       N|\n",
      "|             10|        1107|               Fraud|         A1|           122|2015|    8|   Saturday|  15|  Part Two| WASHINGTON ST|       N|\n",
      "|             11|        1107|               Fraud|UNDISCLOSED|              |2015|    7|  Wednesday|  24|  Part Two|        COBDEN|       N|\n",
      "|             12|         735| Auto Theft Recovery|        C11|           385|2015|   11|     Sunday|  11|     Other|     HALLET ST|       N|\n",
      "|             13|        1102|               Fraud|        C11|           345|2015|   12|     Monday|   4|  Part Two|     DRAPER ST|       N|\n",
      "|             14|        1107|               Fraud|         A7|            20|2015|    7|  Wednesday|   8|  Part Two|      PARIS ST|       N|\n",
      "|             15|        1102|               Fraud|        C11|           342|2015|    9|    Tuesday|  24|  Part Two|    TOPLIFF ST|       N|\n",
      "|             16|        1102|               Fraud|        C11|           365|2015|    9|   Thursday|   9|  Part Two|     SEMONT RD|       N|\n",
      "|             17|         615|Larceny From Moto...|        C11|           366|2015|   10|   Thursday|   8|  Part One|      EDWIN ST|       N|\n",
      "|             18|        3115|  Investigate Person|        D14|           775|2015|    7|  Wednesday|  24|Part Three|WALLINGFORD RD|       N|\n",
      "|             19|        1102|               Fraud|         A1|           101|2015|    8|     Friday|   9|  Part Two|      BROAD ST|       N|\n",
      "|             20|        1102|               Fraud|         D4|           165|2015|   10|   Thursday|  24|  Part Two|     ALBANY ST|       N|\n",
      "+---------------+------------+--------------------+-----------+--------------+----+-----+-----------+----+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 800\n"
     ]
    }
   ],
   "source": [
    "filled_df = df.na.fill('UNDISCLOSED', subset=['DISTRICT'])\n",
    "filled_df.show()\n",
    "print(\"Total data points:\", filled_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('logistic_regression_docs').getOrCreate()\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[19.8534775947478...|[0.99999999761359...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.377398194908...|[1.41321555111056...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-27.401459284891...|[1.25804865126979...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-18.862741612668...|[6.42710509170303...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.483011833009...|[1.27157209200604...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[19.8506078990277...|[0.99999999760673...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.337256674833...|[1.47109814695581...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.595579753418...|[3.08850168102631...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[19.2708803215613...|[0.99999999572670...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[23.6202328360422...|[0.99999999994480...|       0.0|\n",
      "|  1.0|(692,[154,155,156...|[-24.385235147661...|[2.56818872776510...|       1.0|\n",
      "|  0.0|(692,[153,154,155...|[26.3082522490179...|[0.99999999999624...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[25.8329060318703...|[0.99999999999396...|       0.0|\n",
      "|  1.0|(692,[129,130,131...|[-19.794609139086...|[2.53110684529575...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.0260440948067...|[0.99999999926123...|       0.0|\n",
      "|  1.0|(692,[150,151,152...|[-22.764979942873...|[1.29806018790941...|       1.0|\n",
      "|  0.0|(692,[124,125,126...|[21.5049307193954...|[0.99999999954235...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[31.9927184226421...|[0.99999999999998...|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-20.521067180414...|[1.22409115616505...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-22.245377742755...|[2.18250475400332...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load training data. Libsvm is used throughout the Spark documentation, but it's probably not relevant to your dataset. \n",
    "training = spark.read.format(\"libsvm\").load(\"Datasets/sample_libsvm_data.txt\")\n",
    "\n",
    "# Print schema to get a high level view of the schema. Simply label and features. \n",
    "training.printSchema()\n",
    "\n",
    "# Create an instance of the logistic regression model. This is where you specify features/label/prediction columns.\n",
    "# The documentation example includes three parameters. We'll leave those out for now, but they're important to specify in your assignment.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the model. Note that the documentation example doesn't split the data into training/testing/validation sets.\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Instead of printing the coefficients/intercept, let's try to get a high level view of the model output. \n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Print schema again. Note the additional columns. As with other models, we simply want to compare the label (actual value) to the predicted value.\n",
    "trainingSummary.predictions.printSchema()\n",
    "\n",
    "# Visualise the DataFrame. Visually compare label to prediction. What do you see? \n",
    "trainingSummary.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainTest Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[20.8078643965022...|[0.99999999908111...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[32.7111001480420...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[27.9110383683770...|[0.99999999999924...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[23.0134775295046...|[0.99999999989875...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[24.5401191203218...|[0.99999999997800...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[32.8492077698626...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[16.8500827018315...|[0.99999995190476...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[21.9794449196109...|[0.99999999971526...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[20.2502609826300...|[0.99999999839519...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[17.0174607183898...|[0.99999995931721...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[26.0887829388738...|[0.99999999999532...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[-13.303966709494...|[1.66786135604950...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[16.1439008710006...|[0.99999990254752...|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[8.52315661942210...|[0.99980122865431...|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[-19.275896572379...|[4.25191410976761...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-16.206288183980...|[9.15584495656493...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-19.130869786428...|[4.91551262192662...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-21.455959726575...|[4.80612527276353...|       1.0|\n",
      "|  1.0|(692,[125,126,153...|[-21.562630528210...|[4.31984879850662...|       1.0|\n",
      "|  1.0|(692,[125,126,153...|[-21.988767983331...|[2.82097606196274...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's split the data into a training/testing split so we can evalaute the model.\n",
    "lr_train,lr_test = training.randomSplit([0.7,0.3])\n",
    "\n",
    "# Create instance of the logistic regression model.\n",
    "final_model = LogisticRegression()\n",
    "\n",
    "# Now fit the model on a subset of data.\n",
    "fit_final = final_model.fit(lr_train)\n",
    "\n",
    "# And evaluate it against the test data.\n",
    "predictions_and_labels = fit_final.evaluate(lr_test)\n",
    "\n",
    "predictions_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874999999999999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the evaluator (finds area under the curve).\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Evaluate the predictions. \n",
    "my_final_roc = evaluator.evaluate(predictions_and_labels.predictions)\n",
    "\n",
    "# Display the results. \n",
    "my_final_roc\n",
    "\n",
    "# According to this evaluation metric, the area under the curve is 1.0. A perfect fit? Is that realistic?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INCIDENT_NUMBER: integer (nullable = true)\n",
      " |-- OFFENSE_CODE: integer (nullable = true)\n",
      " |-- OFFENSE_CODE_GROUP: string (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- REPORTING_AREA: string (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- HOUR: integer (nullable = true)\n",
      " |-- UCR_PART: string (nullable = true)\n",
      " |-- STREET: string (nullable = true)\n",
      " |-- SHOOTING: string (nullable = true)\n",
      "\n",
      "['INCIDENT_NUMBER', 'OFFENSE_CODE', 'OFFENSE_CODE_GROUP', 'DISTRICT', 'REPORTING_AREA', 'YEAR', 'MONTH', 'DAY_OF_WEEK', 'HOUR', 'UCR_PART', 'STREET', 'SHOOTING']\n"
     ]
    }
   ],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.appName('logistic_regression_adv').getOrCreate()\n",
    "\n",
    "# If you're getting an error with numpy, please type 'sudo pip3 install numpy --user' into the console.\n",
    "# If you're getting an error with another package, type 'sudo pip3 install PACKAGENAME --user'. \n",
    "# Replace PACKAGENAME with the relevant package (such as pandas, etc).\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Import data and print schema - columns is another way to view the data's features.\n",
    "df = spark.read.csv('BostonCrime2.0.csv', header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilising Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INCIDENT_NUMBER</th>\n",
       "      <th>OFFENSE_CODE</th>\n",
       "      <th>OFFENSE_CODE_GROUP</th>\n",
       "      <th>DISTRICT</th>\n",
       "      <th>REPORTING_AREA</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>UCR_PART</th>\n",
       "      <th>STREET</th>\n",
       "      <th>SHOOTING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1102</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>D4</td>\n",
       "      <td>619</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>14</td>\n",
       "      <td>Part Two</td>\n",
       "      <td>WESTLAND AVE</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>619</td>\n",
       "      <td>Larceny</td>\n",
       "      <td>D4</td>\n",
       "      <td>619</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>14</td>\n",
       "      <td>Part One</td>\n",
       "      <td>WESTLAND AVE</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1107</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>E18</td>\n",
       "      <td>486</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>12</td>\n",
       "      <td>Part Two</td>\n",
       "      <td>OAKCREST RD</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1107</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>C6</td>\n",
       "      <td>226</td>\n",
       "      <td>2015</td>\n",
       "      <td>11</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8</td>\n",
       "      <td>Part Two</td>\n",
       "      <td>E FOURTH ST</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2647</td>\n",
       "      <td>Other</td>\n",
       "      <td>D4</td>\n",
       "      <td>129</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>13</td>\n",
       "      <td>Part Two</td>\n",
       "      <td>BERKELEY ST</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   INCIDENT_NUMBER  OFFENSE_CODE OFFENSE_CODE_GROUP DISTRICT REPORTING_AREA  \\\n",
       "0                1          1102              Fraud       D4            619   \n",
       "1                2           619            Larceny       D4            619   \n",
       "2                3          1107              Fraud      E18            486   \n",
       "3                4          1107              Fraud       C6            226   \n",
       "4                5          2647              Other       D4            129   \n",
       "\n",
       "   YEAR  MONTH DAY_OF_WEEK  HOUR  UCR_PART        STREET SHOOTING  \n",
       "0  2015     12      Sunday    14  Part Two  WESTLAND AVE        N  \n",
       "1  2015     12      Sunday    14  Part One  WESTLAND AVE        N  \n",
       "2  2015      7   Wednesday    12  Part Two   OAKCREST RD        N  \n",
       "3  2015     11    Thursday     8  Part Two   E FOURTH ST        N  \n",
       "4  2015     10    Saturday    13  Part Two   BERKELEY ST        N  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas.\n",
    "import pandas as pd\n",
    "\n",
    "# Take the first five rows of data, and visualise.\n",
    "pd.DataFrame(df.take(5), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>INCIDENT_NUMBER</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFFENSE_CODE</th>\n",
       "      <td>1102</td>\n",
       "      <td>619</td>\n",
       "      <td>1107</td>\n",
       "      <td>1107</td>\n",
       "      <td>2647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFFENSE_CODE_GROUP</th>\n",
       "      <td>Fraud</td>\n",
       "      <td>Larceny</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTRICT</th>\n",
       "      <td>D4</td>\n",
       "      <td>D4</td>\n",
       "      <td>E18</td>\n",
       "      <td>C6</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REPORTING_AREA</th>\n",
       "      <td>619</td>\n",
       "      <td>619</td>\n",
       "      <td>486</td>\n",
       "      <td>226</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTH</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUR</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCR_PART</th>\n",
       "      <td>Part Two</td>\n",
       "      <td>Part One</td>\n",
       "      <td>Part Two</td>\n",
       "      <td>Part Two</td>\n",
       "      <td>Part Two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STREET</th>\n",
       "      <td>WESTLAND AVE</td>\n",
       "      <td>WESTLAND AVE</td>\n",
       "      <td>OAKCREST RD</td>\n",
       "      <td>E FOURTH ST</td>\n",
       "      <td>BERKELEY ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOOTING</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0             1            2            3  \\\n",
       "INCIDENT_NUMBER                1             2            3            4   \n",
       "OFFENSE_CODE                1102           619         1107         1107   \n",
       "OFFENSE_CODE_GROUP         Fraud       Larceny        Fraud        Fraud   \n",
       "DISTRICT                      D4            D4          E18           C6   \n",
       "REPORTING_AREA               619           619          486          226   \n",
       "YEAR                        2015          2015         2015         2015   \n",
       "MONTH                         12            12            7           11   \n",
       "DAY_OF_WEEK               Sunday        Sunday    Wednesday     Thursday   \n",
       "HOUR                          14            14           12            8   \n",
       "UCR_PART                Part Two      Part One     Part Two     Part Two   \n",
       "STREET              WESTLAND AVE  WESTLAND AVE  OAKCREST RD  E FOURTH ST   \n",
       "SHOOTING                       N             N            N            N   \n",
       "\n",
       "                              4  \n",
       "INCIDENT_NUMBER               5  \n",
       "OFFENSE_CODE               2647  \n",
       "OFFENSE_CODE_GROUP        Other  \n",
       "DISTRICT                     D4  \n",
       "REPORTING_AREA              129  \n",
       "YEAR                       2015  \n",
       "MONTH                        10  \n",
       "DAY_OF_WEEK            Saturday  \n",
       "HOUR                         13  \n",
       "UCR_PART               Part Two  \n",
       "STREET              BERKELEY ST  \n",
       "SHOOTING                      N  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To visualise the first five columns, simply add transpose. \n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHOOTING</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SHOOTING  count\n",
       "0        Y      3\n",
       "1        N    797"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use group by and count to find out how many data points we have for each class in our predictor. \n",
    "df.groupby('SHOOTING').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INCIDENT_NUMBER</th>\n",
       "      <td>800</td>\n",
       "      <td>400.5</td>\n",
       "      <td>231.08440016582685</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFFENSE_CODE</th>\n",
       "      <td>800</td>\n",
       "      <td>1949.24875</td>\n",
       "      <td>1095.3304308812276</td>\n",
       "      <td>301</td>\n",
       "      <td>3831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <td>800</td>\n",
       "      <td>2016.50375</td>\n",
       "      <td>1.1209623692572204</td>\n",
       "      <td>2015</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTH</th>\n",
       "      <td>800</td>\n",
       "      <td>8.2025</td>\n",
       "      <td>2.677770867909777</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUR</th>\n",
       "      <td>800</td>\n",
       "      <td>15.49</td>\n",
       "      <td>6.011661591762074</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0           1                   2     3     4\n",
       "summary          count        mean              stddev   min   max\n",
       "INCIDENT_NUMBER    800       400.5  231.08440016582685     1   800\n",
       "OFFENSE_CODE       800  1949.24875  1095.3304308812276   301  3831\n",
       "YEAR               800  2016.50375  1.1209623692572204  2015  2018\n",
       "MONTH              800      8.2025   2.677770867909777     1    12\n",
       "HOUR               800       15.49   6.011661591762074     1    24"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a for loop to find all columns that belong to the integer data type. \n",
    "# no pritical meaning for this step, my dataset belongs to categorical....\n",
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
    "\n",
    "# Selecting the numeric features, generating summary statistics, and converting to a Pandas DataFrame.\n",
    "df.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INCIDENT_NUMBER: integer (nullable = true)\n",
      " |-- OFFENSE_CODE: integer (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- REPORTING_AREA: string (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- HOUR: integer (nullable = true)\n",
      " |-- STREET: string (nullable = true)\n",
      " |-- SHOOTING: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now that we've carried out some basic data exploration, let's select the relevant features. Day/month have been excluded as they're irrelevant.\n",
    "# It’s obvious that there aren’t highly correlated numeric variables. Therefore, we will keep all of them for the model. However, OFFENSE_CODE_GROUP and UCR_PART columns are not really useful, we will remove these two columns.\n",
    "df = df.select('INCIDENT_NUMBER','OFFENSE_CODE','DISTRICT','REPORTING_AREA','YEAR','MONTH','DAY_OF_WEEK','HOUR','STREET','SHOOTING')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OneHotEncoderEstimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-21925f3b1332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoderEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcategoricalColumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'OFFENSE_CODE_GROUP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISTRICT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REPORTING_AREA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DAY_OF_WEEK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UCR_PART'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'STREET'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcategoricalCol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategoricalColumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstringIndexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategoricalCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategoricalCol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OneHotEncoderEstimator'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 76, in __del__\n",
      "    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n",
      "AttributeError: 'OneHotEncoder' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['OFFENSE_CODE_GROUP', 'DISTRICT', 'REPORTING_AREA', 'DAY_OF_WEEK', 'UCR_PART', 'STREET']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'SHOOTING', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['INCIDENT_NUMBER', 'OFFENSE_CODE', 'YEAR', 'MONTH', 'HOUR']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'inputCol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-230cce65f922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                                        \u001b[0;34m'REPORTING_AREAVec'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DAY_OF_WEEKVec'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                        \u001b[0;34m'UCR_PARTVec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'STREETVec'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'INCIDENT_NUMBER'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                        'OFFENSE_CODE','YEAR','MONTH','HOUR'], outputCol=\"features\")\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'inputCol'"
     ]
    }
   ],
   "source": [
    "# Import the relevant packages.\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\n",
    "\n",
    "# First create a string indexer which converts every string into a number, such as male = 0 and female = 1.\n",
    "# A number will be assigned to every category in the column.\n",
    "OFFENSE_CODE_GROUP_indexer = StringIndexer(inputCol='OFFENSE_CODE_GROUP',outputCol='OFFENSE_CODE_GROUPIndex')\n",
    "DISTRICT_indexer = StringIndexer(inputCol='DISTRICT',outputCol='DISTRICTIndex')\n",
    "REPORTING_AREA_indexer = StringIndexer(inputCol='REPORTING_AREA',outputCol='REPORTING_AREAIndex')\n",
    "DAY_OF_WEEK_indexer = StringIndexer(inputCol='DAY_OF_WEEK',outputCol='DAY_OF_WEEKIndex')\n",
    "UCR_PART_indexer = StringIndexer(inputCol='UCR_PART',outputCol='UCR_PARTIndex')\n",
    "STREET_indexer = StringIndexer(inputCol='STREET',outputCol='STREETIndex')\n",
    "SHOOTING_indexer = StringIndexer(inputCol='SHOOTING',outputCol='label')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "# Multiple columns are collapsed into one. \n",
    "# This makes it easier to process when you have multiple classes.\n",
    "OFFENSE_CODE_GROUP_encoder = OneHotEncoder(inputCol='OFFENSE_CODE_GROUPIndex',outputCol='OFFENSE_CODE_GROUPVec')\n",
    "DISTRICT_encoder = OneHotEncoder(inputCol='DISTRICTIndex',outputCol='DISTRICTVec')\n",
    "REPORTING_AREA_encoder = OneHotEncoder(inputCol='REPORTING_AREAIndex',outputCol='REPORTING_AREAVec')\n",
    "DAY_OF_WEEK_encoder = OneHotEncoder(inputCol='DAY_OF_WEEKIndex',outputCol='DAY_OF_WEEKVec')\n",
    "UCR_PART_encoder = OneHotEncoder(inputCol='UCR_PARTIndex',outputCol='UCR_PARTVec')\n",
    "STREET_encoder = OneHotEncoder(inputCol='STREETIndex',outputCol='STREETVec')\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['OFFENSE_CODE_GROUPVec','DISTRICTVec',\n",
    "                                       'REPORTING_AREAVec','DAY_OF_WEEKVec',\n",
    "                                       'UCR_PARTVec', 'STREETVec','INCIDENT_NUMBER',\n",
    "                                       'OFFENSE_CODE','YEAR','MONTH','HOUR'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o236.transform.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:219)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$2.apply(Metadata.scala:207)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$2.apply(Metadata.scala:207)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:207)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map3.foreach(Map.scala:161)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat scala.util.hashing.MurmurHash3.unorderedHash(MurmurHash3.scala:91)\n\tat scala.util.hashing.MurmurHash3$.mapHash(MurmurHash3.scala:222)\n\tat scala.collection.GenMapLike$class.hashCode(GenMapLike.scala:35)\n\tat scala.collection.AbstractMap.hashCode(Map.scala:59)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:116)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat scala.util.hashing.MurmurHash3.unorderedHash(MurmurHash3.scala:91)\n\tat scala.util.hashing.MurmurHash3$.mapHash(MurmurHash3.scala:222)\n\tat scala.collection.GenMapLike$class.hashCode(GenMapLike.scala:35)\n\tat scala.collection.AbstractMap.hashCode(Map.scala:59)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata._hashCode$lzycompute(Metadata.scala:107)\n\tat org.apache.spark.sql.types.Metadata._hashCode(Metadata.scala:107)\n\tat org.apache.spark.sql.types.Metadata.hashCode(Metadata.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.AttributeReference.hashCode(namedExpressions.scala:249)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat scala.collection.immutable.HashSet.elemHashCode(HashSet.scala:177)\n\tat scala.collection.immutable.HashSet.computeHash(HashSet.scala:186)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:84)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:35)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:22)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.generic.Growable$class.loop$1(Growable.scala:53)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:57)\n\tat scala.collection.mutable.SetBuilder.$plus$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.TraversableLike$class.to(TraversableLike.scala:590)\n\tat scala.collection.AbstractTraversable.to(Traversable.scala:104)\n\tat scala.collection.TraversableOnce$class.toSet(TraversableOnce.scala:304)\n\tat scala.collection.AbstractTraversable.toSet(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild$lzycompute(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:334)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$32.applyOrElse(Analyzer.scala:2031)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$32.applyOrElse(Analyzer.scala:2027)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2027)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2026)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(ExpressionEncoder.scala:258)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:209)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2845)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1131)\n\tat org.apache.spark.ml.feature.StringIndexerModel.transform(StringIndexer.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-58e5c6f27a09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                             STREET_encoder, assembler])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpipeline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o236.transform.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:219)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$2.apply(Metadata.scala:207)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$2.apply(Metadata.scala:207)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:207)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map3.foreach(Map.scala:161)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat scala.util.hashing.MurmurHash3.unorderedHash(MurmurHash3.scala:91)\n\tat scala.util.hashing.MurmurHash3$.mapHash(MurmurHash3.scala:222)\n\tat scala.collection.GenMapLike$class.hashCode(GenMapLike.scala:35)\n\tat scala.collection.AbstractMap.hashCode(Map.scala:59)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:116)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat scala.util.hashing.MurmurHash3.unorderedHash(MurmurHash3.scala:91)\n\tat scala.util.hashing.MurmurHash3$.mapHash(MurmurHash3.scala:222)\n\tat scala.collection.GenMapLike$class.hashCode(GenMapLike.scala:35)\n\tat scala.collection.AbstractMap.hashCode(Map.scala:59)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata._hashCode$lzycompute(Metadata.scala:107)\n\tat org.apache.spark.sql.types.Metadata._hashCode(Metadata.scala:107)\n\tat org.apache.spark.sql.types.Metadata.hashCode(Metadata.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.AttributeReference.hashCode(namedExpressions.scala:249)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat scala.collection.immutable.HashSet.elemHashCode(HashSet.scala:177)\n\tat scala.collection.immutable.HashSet.computeHash(HashSet.scala:186)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:84)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:35)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:22)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.generic.Growable$class.loop$1(Growable.scala:53)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:57)\n\tat scala.collection.mutable.SetBuilder.$plus$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.TraversableLike$class.to(TraversableLike.scala:590)\n\tat scala.collection.AbstractTraversable.to(Traversable.scala:104)\n\tat scala.collection.TraversableOnce$class.toSet(TraversableOnce.scala:304)\n\tat scala.collection.AbstractTraversable.toSet(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild$lzycompute(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:334)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:245)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$32.applyOrElse(Analyzer.scala:2031)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$32.applyOrElse(Analyzer.scala:2027)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2027)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2026)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(ExpressionEncoder.scala:258)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:209)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2845)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1131)\n\tat org.apache.spark.ml.feature.StringIndexerModel.transform(StringIndexer.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline = Pipeline(stages=[OFFENSE_CODE_GROUP_indexer, DISTRICT_indexer, REPORTING_AREA_indexer, DAY_OF_WEEK_indexer,\n",
    "                            UCR_PART_indexer, STREET_indexer, SHOOTING_indexer, OFFENSE_CODE_GROUP_encoder, \n",
    "                            DISTRICT_encoder, REPORTING_AREA_encoder, DAY_OF_WEEK_encoder, UCR_PART_encoder, \n",
    "                            STREET_encoder, assembler])\n",
    "\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "\n",
    "pipe_df = pipeline_model.transform(df)\n",
    "\n",
    "\n",
    "pipe_df = pipe_df.select('label', 'features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cbf3dbef7d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Split our data. Note that the new DataFrame is being used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Dataset Count: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Dataset Count: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Split our data. Note that the new DataFrame is being used.\n",
    "train_data, test_data = pipe_df.randomSplit([0.7,0.3])\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "# Instantiate the model.\n",
    "lr_model = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "\n",
    "# Fit the model.\n",
    "lr_model = lr_model.fit(train_data)\n",
    "\n",
    "# And evaluate the model using the test data.\n",
    "results = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0140beec1313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Visualising the coefficients. Sort from lowest to highest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Plot the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualising the coefficients. Sort from lowest to highest.\n",
    "beta = np.sort(lr_model.coefficients)\n",
    "\n",
    "# Plot the data.\n",
    "plt.plot(beta)\n",
    "\n",
    "# Add a label to the data.\n",
    "plt.ylabel('Beta Coefficients')\n",
    "\n",
    "# Show the graph. \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROCAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-7f858d1fa337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's get a summary of the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert the DataFrame to a Pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mROC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's get a summary of the data.\n",
    "training_summary = lr_model.summary\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame.\n",
    "ROC = training_summary.roc.toPandas()\n",
    "\n",
    "# Plot the true positive and false positive rates.\n",
    "plt.plot(ROC['FPR'],ROC['TPR'])\n",
    "\n",
    "# Define the labels.\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC statistic. \n",
    "print('Area Under the Curve: ' + str(training_summary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PrecisionRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-8ca665b9b751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert DataFrame to Pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot model recall and precision.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_summary' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to Pandas DataFrame.\n",
    "pr = training_summary.pr.toPandas()\n",
    "\n",
    "# Plot model recall and precision.\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "\n",
    "# Define the labels and show the graph. \n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
